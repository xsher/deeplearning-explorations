{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author: Sherly Sherly**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Objectives:\n",
    "- Applications of different preprocessing methods\n",
    "    - lemmatize/non-lemmatize\n",
    "    - POS tags\n",
    "    - unigram/bigram/skip-gram\n",
    "- Application of different techniques\n",
    "    - The objective is to evaluate what is the best way to do the word representations in order to calculate the similarity\n",
    "    - Examples:\n",
    "        - Bag-of-words\n",
    "        - TF-IDF\n",
    "        - Latent Semantic Indexing\n",
    "        - Latent Dirichlet Allocation\n",
    "        - Word2Vec\n",
    "        - FastText\n",
    "        - FastText pre-trained\n",
    "\n",
    "- Application of same technique on different problems \n",
    "    - Question - Answer\n",
    "    - Question - Question\n",
    "\n",
    "References:\n",
    "- https://staff.fnwi.uva.nl/m.derijke/wp-content/papercite-data/pdf/kenter-short-2015.pdf\n",
    "- https://pdfs.semanticscholar.org/d632/3544c5c103c8c0094202f38922c12db50d65.pdf\n",
    "- https://arxiv.org/pdf/1802.05667.pdf\n",
    "- https://github.com/tbmihailov/semeval2016-task3-cqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning points from the different papers\n",
    "\n",
    "<u>Question Condensing Networks for Answer Selection in Community Question Answering</u>  \n",
    "Wei Wu, Xu Sun, Houfeng Wang\n",
    "\n",
    "- Specifically trained GloVe vectors can model word interactions more precisely +  Character embedding has proven to be very useful for out-of-vocabulary (OOV) words, so it is especially suitable for noisy web text in CQA. We concatenate these two embedding vectors for every word to generate word-level embeddings\n",
    "-  We propose to treat the question subject and the question body separately in community question answering. We treat the question subject as the primary part of the question, and aggregate the question body information based on similarity and disparity with the question subject.\n",
    "- We introduce a new method that uses the multi-dimensional attention mechanism to align question-answer pair. With this attention mechanism, the interaction between questions and answers can be learned more accurately.\n",
    "- Our proposed Question Condensing Networks (QCN) achieves the state-of-the-art performance on two SemEval CQA datasets, outperforming all exisiting SOTA models by a large margin, which demonstrates the effectiveness of our model.\n",
    "- We propose to cheat the question subject as the primary part of the question representation, and aggregate question body information from two perspectives: similarity and disparity with the question subject.\n",
    "\n",
    "\n",
    "<u>KeLP at SemEval-2017 Task 3: Learning Pairwise Patterns in Community Question Answering</u>  \n",
    "Simone Filice, Giovanni Da San Martino and Alessandro Moschitti\n",
    "- We modeled the three subtasks as binary classification problems: kernel-based classifiers are trained and the classification score is used to sort the instances and produce the final ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    " \n",
    "## Subtask B: Question-Question Similarity\n",
    "**Given**  \n",
    "- a new question (aka original question) and\n",
    "- the set of the first 10 related questions (retrieved by a search engine), \n",
    "\n",
    "rerank the related questions according to their similarity with respect to the original question. In this case, we will consider the \"PerfectMatch\" and \"Relevant\" questions both as good (i.e., we will not distinguish between them and we will consider them both \"Relevant\"), and they should be ranked above the \"Irrelevant\" questions. The gold labels for this subtask are contained in the RELQ_RELEVANCE2ORGQ field of the XML file. See the README file for a detailed explanation of their meaning. Again, this is not a classification task; it is a ranking task.\n",
    " \n",
    "**Evaluation:**  \n",
    "As in subtask A, the official scorer will provide a number of evaluation measures to assess the quality of the output of a system (see the tools page), but the official evaluation measure towards which all systems will be evaluated and ranked is MAP using the 10 ranked questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_format'></a>\n",
    "\n",
    "## 1. Data Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given dataset for the challenge is in XML format. We will first need to parse the relevant information from the XML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapathprefix = \"v3.2/train/\"\n",
    "data_paths = ['SemEval2016-Task3-CQA-QL-train-part1-with-multiline.xml',\n",
    "             'SemEval2016-Task3-CQA-QL-train-part2-with-multiline.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ElementTree\n",
    "\n",
    "def XMLParser(filepath):\n",
    "    # construct the Element Tree and get the root\n",
    "    tree = ElementTree.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    question_list = []\n",
    "\n",
    "    for org_question in root.findall('OrgQuestion'):\n",
    "        question_dict = {}\n",
    "\n",
    "        question_dict['ORGQ_ID'] = org_question.attrib[\"ORGQ_ID\"]\n",
    "        question_dict['org_subject'] = org_question.find(\"OrgQSubject\").text\n",
    "        question_dict['org_question'] = org_question.find(\"OrgQBody\").text\n",
    "\n",
    "        thread = org_question.find('Thread')\n",
    "        rel_question = thread.find('RelQuestion')\n",
    "\n",
    "        question_dict['threadId'] = rel_question.attrib['RELQ_ID']\n",
    "        question_dict['subject'] = rel_question.find('RelQSubject').text\n",
    "        question_dict['question'] = rel_question.find('RelQBody').text\n",
    "\n",
    "        # if there are no question body, use subject\n",
    "        if(question_dict['question'] is None):\n",
    "            question_dict['question'] = question_dict['subject']\n",
    "\n",
    "        question_dict['relevance'] = rel_question.attrib['RELQ_RELEVANCE2ORGQ']\n",
    "\n",
    "        question_list.append(question_dict)\n",
    "\n",
    "    return question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for p in data_paths:\n",
    "    data += XMLParser(datapathprefix + p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_clean'></a>\n",
    "\n",
    "\n",
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# suppress warning from bs4\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# Clean extraction of the information\n",
    "def stripHtml(txt):\n",
    "    if txt is None:\n",
    "        return\n",
    "    else:\n",
    "        soup = BeautifulSoup(txt, \"lxml\")\n",
    "        return ''.join(BeautifulSoup(soup.get_text(), \"lxml\").findAll(text=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopw = stopwords.words('english')\n",
    "stopw += ['hi', 'hello']\n",
    "\n",
    "def merge_acronym(s):\n",
    "    r = re.compile(r'(?:(?<=\\.|\\s)[A-Z]\\.)+')\n",
    "    acronyms = r.findall(s)\n",
    "    \n",
    "    for w in acronyms:\n",
    "        s = s.replace(w, w.replace('.', ''))\n",
    "        \n",
    "    return s\n",
    "\n",
    "def clean_text(doc):\n",
    "\n",
    "    # determine which ones are required for this task\n",
    "    doc = stripHtml(doc)\n",
    "    doc = re.sub('[^A-Za-z ]+', \" \", doc)\n",
    "\n",
    "    doc = doc.replace(\"-\", \"\")\n",
    "    doc = doc.lower()\n",
    "    \n",
    "    # Some other potential cleaning to do\n",
    "    # doc = doc.replace(\"...\", \"\")\n",
    "    # doc = doc.replace(\"Mr.\", \"Mr\").replace(\"Mrs.\", \"Mrs\")    \n",
    "    # doc = merge_acronym(doc)\n",
    "    \n",
    "    # Remove whitespace\n",
    "    doc = ' '.join(doc.split())\n",
    "    return doc\n",
    "\n",
    "def remove_stop_words(doc):\n",
    "    doc = ' '.join([i for i in doc.split() if i not in stopw])\n",
    "    return doc\n",
    "\n",
    "def clean(doc):\n",
    "    doc = clean_text(doc)\n",
    "    return remove_stop_words(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORGQ_ID</th>\n",
       "      <th>org_subject</th>\n",
       "      <th>org_question</th>\n",
       "      <th>threadId</th>\n",
       "      <th>subject</th>\n",
       "      <th>question</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Massage oil</td>\n",
       "      <td>Where I can buy good oil for massage?</td>\n",
       "      <td>Q1_R1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>is there any place i can find scented massage ...</td>\n",
       "      <td>PerfectMatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Massage oil</td>\n",
       "      <td>Where I can buy good oil for massage?</td>\n",
       "      <td>Q1_R6</td>\n",
       "      <td>Philipino Massage center</td>\n",
       "      <td>Hi,Can any one tell me a place where i can hav...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Massage oil</td>\n",
       "      <td>Where I can buy good oil for massage?</td>\n",
       "      <td>Q1_R8</td>\n",
       "      <td>Best place for massage</td>\n",
       "      <td>&amp;lt;p&amp;gt;\\nTell me, where is the best place to...</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Massage oil</td>\n",
       "      <td>Where I can buy good oil for massage?</td>\n",
       "      <td>Q1_R10</td>\n",
       "      <td>body massage</td>\n",
       "      <td>hi there, i can see a lot of massage center he...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Massage oil</td>\n",
       "      <td>Where I can buy good oil for massage?</td>\n",
       "      <td>Q1_R22</td>\n",
       "      <td>What attracts you more ?</td>\n",
       "      <td>What attracts you more ?</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ORGQ_ID  org_subject                           org_question threadId  \\\n",
       "0      Q1  Massage oil  Where I can buy good oil for massage?    Q1_R1   \n",
       "1      Q1  Massage oil  Where I can buy good oil for massage?    Q1_R6   \n",
       "2      Q1  Massage oil  Where I can buy good oil for massage?    Q1_R8   \n",
       "3      Q1  Massage oil  Where I can buy good oil for massage?   Q1_R10   \n",
       "4      Q1  Massage oil  Where I can buy good oil for massage?   Q1_R22   \n",
       "\n",
       "                    subject  \\\n",
       "0              massage oil    \n",
       "1  Philipino Massage center   \n",
       "2    Best place for massage   \n",
       "3              body massage   \n",
       "4  What attracts you more ?   \n",
       "\n",
       "                                            question     relevance  \n",
       "0  is there any place i can find scented massage ...  PerfectMatch  \n",
       "1  Hi,Can any one tell me a place where i can hav...      Relevant  \n",
       "2  &lt;p&gt;\\nTell me, where is the best place to...    Irrelevant  \n",
       "3  hi there, i can see a lot of massage center he...      Relevant  \n",
       "4                           What attracts you more ?    Irrelevant  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question'] = df.question.apply(lambda x: clean(x))\n",
    "df['org_question'] = df.org_question.apply(lambda x: clean(x))\n",
    "df['subject'] = df.subject.apply(lambda x: clean(x))\n",
    "df['org_subject'] = df.org_subject.apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORGQ_ID</th>\n",
       "      <th>org_subject</th>\n",
       "      <th>org_question</th>\n",
       "      <th>threadId</th>\n",
       "      <th>subject</th>\n",
       "      <th>question</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>place find scented massage oils qatar</td>\n",
       "      <td>PerfectMatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R6</td>\n",
       "      <td>philipino massage center</td>\n",
       "      <td>one tell place good massage drom philipinies y...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R8</td>\n",
       "      <td>best place massage</td>\n",
       "      <td>tell best place go massage mind want spend qr ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R10</td>\n",
       "      <td>body massage</td>\n",
       "      <td>see lot massage center dont one better someone...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R22</td>\n",
       "      <td>attracts</td>\n",
       "      <td>attracts</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ORGQ_ID  org_subject          org_question threadId  \\\n",
       "0      Q1  massage oil  buy good oil massage    Q1_R1   \n",
       "1      Q1  massage oil  buy good oil massage    Q1_R6   \n",
       "2      Q1  massage oil  buy good oil massage    Q1_R8   \n",
       "3      Q1  massage oil  buy good oil massage   Q1_R10   \n",
       "4      Q1  massage oil  buy good oil massage   Q1_R22   \n",
       "\n",
       "                    subject  \\\n",
       "0               massage oil   \n",
       "1  philipino massage center   \n",
       "2        best place massage   \n",
       "3              body massage   \n",
       "4                  attracts   \n",
       "\n",
       "                                            question     relevance  \n",
       "0              place find scented massage oils qatar  PerfectMatch  \n",
       "1  one tell place good massage drom philipinies y...      Relevant  \n",
       "2  tell best place go massage mind want spend qr ...    Irrelevant  \n",
       "3  see lot massage center dont one better someone...      Relevant  \n",
       "4                                           attracts    Irrelevant  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='methodologies'></a>\n",
    "\n",
    "## 3. Methodologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tdidf'></a>\n",
    "### 3.1 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "def fit_tfidf(data):\n",
    "    count_vect = CountVectorizer()\n",
    "    count_vect = count_vect.fit(data)\n",
    "\n",
    "    freq_term_matrix = count_vect.transform(data)\n",
    "\n",
    "    feature_names = count_vect.get_feature_names()\n",
    "\n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf.fit(freq_term_matrix)\n",
    "    \n",
    "    return count_vect, tfidf, feature_names\n",
    "\n",
    "\n",
    "def retrieve_doc_matrix(doc, count_vect, tfidf):\n",
    "    # get the freq term matrix of the doc\n",
    "    freq_term_matrix = count_vect.transform([doc])\n",
    "    # get the tfidf matrix\n",
    "    tfidf_matrix = tfidf.transform(freq_term_matrix)\n",
    "    # dense form of the matrix\n",
    "    dense_mat = tfidf_matrix.todense()\n",
    "    # return doc_matrix\n",
    "    return dense_mat.tolist()[0]\n",
    "\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(p*q for p,q in zip(vector1, vector2))\n",
    "    magnitude = math.sqrt(\n",
    "            sum([val**2 for val in vector1])) * \\\n",
    "            math.sqrt(sum([val**2 for val in vector2])\n",
    "        )\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    return dot_product/magnitude\n",
    "\n",
    "\n",
    "def rank_similarity(doc, other_docs, count_vect, tfidf):\n",
    "    doc_sim_scores = []\n",
    "    doc_mat = retrieve_doc_matrix(doc, count_vect, tfidf)\n",
    "    \n",
    "    for d in other_docs:\n",
    "        d_mat = retrieve_doc_matrix(d, count_vect, tfidf)\n",
    "        doc_sim_scores.append((d, cosine_similarity(doc_mat, d_mat)))\n",
    "        \n",
    "    sorted_docs = sorted(doc_sim_scores,\n",
    "                         key=lambda tup: tup[1],\n",
    "                         reverse=True)\n",
    "    \n",
    "    return sorted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_train = df.org_question.values + df.question.values\n",
    "cnt_vect, tfidf_fit, f_names = fit_tfidf(question_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gen_sim_scores(sent1, sent2, count_vect, tfidf):\n",
    "    sent1_mat = retrieve_doc_matrix(sent1, count_vect, tfidf)\n",
    "    sent2_mat = retrieve_doc_matrix(sent2, count_vect, tfidf)\n",
    "    return cosine_similarity(sent1_mat, sent2_mat)\n",
    "\n",
    "df['score'] = df.apply(\n",
    "        lambda x: gen_sim_scores(x['org_question'], x['question'],\n",
    "               cnt_vect, tfidf_fit), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORGQ_ID</th>\n",
       "      <th>org_subject</th>\n",
       "      <th>org_question</th>\n",
       "      <th>threadId</th>\n",
       "      <th>subject</th>\n",
       "      <th>question</th>\n",
       "      <th>relevance</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>place find scented massage oils qatar</td>\n",
       "      <td>PerfectMatch</td>\n",
       "      <td>0.320963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R6</td>\n",
       "      <td>philipino massage center</td>\n",
       "      <td>one tell place good massage drom philipinies y...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.318438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R8</td>\n",
       "      <td>best place massage</td>\n",
       "      <td>tell best place go massage mind want spend qr ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.234103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R10</td>\n",
       "      <td>body massage</td>\n",
       "      <td>see lot massage center dont one better someone...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.483658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R22</td>\n",
       "      <td>attracts</td>\n",
       "      <td>attracts</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ORGQ_ID  org_subject          org_question threadId  \\\n",
       "0      Q1  massage oil  buy good oil massage    Q1_R1   \n",
       "1      Q1  massage oil  buy good oil massage    Q1_R6   \n",
       "2      Q1  massage oil  buy good oil massage    Q1_R8   \n",
       "3      Q1  massage oil  buy good oil massage   Q1_R10   \n",
       "4      Q1  massage oil  buy good oil massage   Q1_R22   \n",
       "\n",
       "                    subject  \\\n",
       "0               massage oil   \n",
       "1  philipino massage center   \n",
       "2        best place massage   \n",
       "3              body massage   \n",
       "4                  attracts   \n",
       "\n",
       "                                            question     relevance     score  \n",
       "0              place find scented massage oils qatar  PerfectMatch  0.320963  \n",
       "1  one tell place good massage drom philipinies y...      Relevant  0.318438  \n",
       "2  tell best place go massage mind want spend qr ...    Irrelevant  0.234103  \n",
       "3  see lot massage center dont one better someone...      Relevant  0.483658  \n",
       "4                                           attracts    Irrelevant  0.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='word2vec'></a>\n",
    "### 3.2 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from gensim.models import word2vec\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_format = [x.split() for x in question_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 18:40:55,850 : INFO : collecting all words and their counts\n",
      "2019-11-13 18:40:55,859 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-13 18:40:56,198 : INFO : collected 9434 word types from a corpus of 104931 raw words and 2669 sentences\n",
      "2019-11-13 18:40:56,201 : INFO : Loading a fresh vocabulary\n",
      "2019-11-13 18:40:56,352 : INFO : min_count=2 retains 4707 unique words (49% of original 9434, drops 4727)\n",
      "2019-11-13 18:40:56,364 : INFO : min_count=2 leaves 100204 word corpus (95% of original 104931, drops 4727)\n",
      "2019-11-13 18:40:56,484 : INFO : deleting the raw counts dictionary of 9434 items\n",
      "2019-11-13 18:40:56,527 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2019-11-13 18:40:56,550 : INFO : downsampling leaves estimated 90805 word corpus (90.6% of prior 100204)\n",
      "2019-11-13 18:40:56,600 : INFO : estimated required memory for 4707 words and 300 dimensions: 13650300 bytes\n",
      "2019-11-13 18:40:56,602 : INFO : resetting layer weights\n",
      "2019-11-13 18:40:57,452 : INFO : training model with 3 workers on 4707 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-11-13 18:40:57,940 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-13 18:40:58,018 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-13 18:40:58,085 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-13 18:40:58,086 : INFO : EPOCH - 1 : training on 104931 raw words (90866 effective words) took 0.5s, 165633 effective words/s\n",
      "2019-11-13 18:40:58,455 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-13 18:40:58,501 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-13 18:40:58,570 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-13 18:40:58,572 : INFO : EPOCH - 2 : training on 104931 raw words (90815 effective words) took 0.5s, 192981 effective words/s\n",
      "2019-11-13 18:40:58,894 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-13 18:40:58,975 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-13 18:40:59,006 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-13 18:40:59,007 : INFO : EPOCH - 3 : training on 104931 raw words (90841 effective words) took 0.4s, 216114 effective words/s\n",
      "2019-11-13 18:40:59,318 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-13 18:40:59,337 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-13 18:40:59,418 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-13 18:40:59,421 : INFO : EPOCH - 4 : training on 104931 raw words (90855 effective words) took 0.4s, 229669 effective words/s\n",
      "2019-11-13 18:40:59,723 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-13 18:40:59,779 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-13 18:40:59,839 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-13 18:40:59,842 : INFO : EPOCH - 5 : training on 104931 raw words (90836 effective words) took 0.4s, 222302 effective words/s\n",
      "2019-11-13 18:40:59,843 : INFO : training on a 524655 raw words (454213 effective words) took 2.4s, 190785 effective words/s\n"
     ]
    }
   ],
   "source": [
    "w2v_model = word2vec.Word2Vec(w2v_format, min_count=2, size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherly/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2019-11-13 18:40:59,862 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('away', 0.9999431371688843),\n",
       " ('im', 0.9999403953552246),\n",
       " ('well', 0.9999372959136963),\n",
       " ('bit', 0.9999366402626038),\n",
       " ('baggage', 0.9999358654022217),\n",
       " ('world', 0.9999357461929321),\n",
       " ('quite', 0.9999347925186157),\n",
       " ('customer', 0.9999324083328247),\n",
       " ('found', 0.9999318718910217),\n",
       " ('little', 0.9999315738677979)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(['beach'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995837211608887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherly/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "index2word_set = set(w2v_model.wv.index2word)\n",
    "\n",
    "## check what is the difference between the two methods\n",
    "\n",
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec\n",
    "\n",
    "s1_afv = avg_feature_vector('this is a sentence a a a one', model=w2v_model, num_features=300, index2word_set=index2word_set)\n",
    "s2_afv = avg_feature_vector('this is also sentence', model=w2v_model, num_features=300, index2word_set=index2word_set)\n",
    "sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_sim_score(sent1, sent2):\n",
    "    sent1_mat = avg_feature_vector(sent1, model=w2v_model,\n",
    "                                   num_features=300,\n",
    "                                   index2word_set=index2word_set)\n",
    "    sent2_mat = avg_feature_vector(sent2, model=w2v_model,\n",
    "                                   num_features=300,\n",
    "                                   index2word_set=index2word_set)\n",
    "    \n",
    "    return cosine_similarity(sent1_mat, sent2_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherly/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "df['w2v_sim'] = df.apply(lambda x: get_w2v_sim_score(\n",
    "    x['org_question'], x['question']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORGQ_ID</th>\n",
       "      <th>org_subject</th>\n",
       "      <th>org_question</th>\n",
       "      <th>threadId</th>\n",
       "      <th>subject</th>\n",
       "      <th>question</th>\n",
       "      <th>relevance</th>\n",
       "      <th>score</th>\n",
       "      <th>w2v_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>place find scented massage oils qatar</td>\n",
       "      <td>PerfectMatch</td>\n",
       "      <td>0.320963</td>\n",
       "      <td>0.999116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R6</td>\n",
       "      <td>philipino massage center</td>\n",
       "      <td>one tell place good massage drom philipinies y...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.318438</td>\n",
       "      <td>0.999398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R8</td>\n",
       "      <td>best place massage</td>\n",
       "      <td>tell best place go massage mind want spend qr ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.234103</td>\n",
       "      <td>0.999021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R10</td>\n",
       "      <td>body massage</td>\n",
       "      <td>see lot massage center dont one better someone...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.483658</td>\n",
       "      <td>0.999651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R22</td>\n",
       "      <td>attracts</td>\n",
       "      <td>attracts</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R25</td>\n",
       "      <td>got joking seen shop downtown manama</td>\n",
       "      <td>img assist nid title placenta cream desc link ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R27</td>\n",
       "      <td>blackheads</td>\n",
       "      <td>suggestions get rid</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R32</td>\n",
       "      <td>get tea tree oil</td>\n",
       "      <td>someone please advise husband wants get tea tr...</td>\n",
       "      <td>PerfectMatch</td>\n",
       "      <td>0.184031</td>\n",
       "      <td>0.998489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R43</td>\n",
       "      <td>strong migraine pain</td>\n",
       "      <td>plz help living hell days tried kind medicine ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R46</td>\n",
       "      <td>garlic oil</td>\n",
       "      <td>someone please tell find garlic oil qatar hear...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.218388</td>\n",
       "      <td>0.999413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ORGQ_ID  org_subject          org_question threadId  \\\n",
       "0      Q1  massage oil  buy good oil massage    Q1_R1   \n",
       "1      Q1  massage oil  buy good oil massage    Q1_R6   \n",
       "2      Q1  massage oil  buy good oil massage    Q1_R8   \n",
       "3      Q1  massage oil  buy good oil massage   Q1_R10   \n",
       "4      Q1  massage oil  buy good oil massage   Q1_R22   \n",
       "5      Q1  massage oil  buy good oil massage   Q1_R25   \n",
       "6      Q1  massage oil  buy good oil massage   Q1_R27   \n",
       "7      Q1  massage oil  buy good oil massage   Q1_R32   \n",
       "8      Q1  massage oil  buy good oil massage   Q1_R43   \n",
       "9      Q1  massage oil  buy good oil massage   Q1_R46   \n",
       "\n",
       "                                subject  \\\n",
       "0                           massage oil   \n",
       "1              philipino massage center   \n",
       "2                    best place massage   \n",
       "3                          body massage   \n",
       "4                              attracts   \n",
       "5  got joking seen shop downtown manama   \n",
       "6                            blackheads   \n",
       "7                      get tea tree oil   \n",
       "8                  strong migraine pain   \n",
       "9                            garlic oil   \n",
       "\n",
       "                                            question     relevance     score  \\\n",
       "0              place find scented massage oils qatar  PerfectMatch  0.320963   \n",
       "1  one tell place good massage drom philipinies y...      Relevant  0.318438   \n",
       "2  tell best place go massage mind want spend qr ...    Irrelevant  0.234103   \n",
       "3  see lot massage center dont one better someone...      Relevant  0.483658   \n",
       "4                                           attracts    Irrelevant  0.000000   \n",
       "5  img assist nid title placenta cream desc link ...    Irrelevant  0.000000   \n",
       "6                                suggestions get rid    Irrelevant  0.000000   \n",
       "7  someone please advise husband wants get tea tr...  PerfectMatch  0.184031   \n",
       "8  plz help living hell days tried kind medicine ...    Irrelevant  0.000000   \n",
       "9  someone please tell find garlic oil qatar hear...    Irrelevant  0.218388   \n",
       "\n",
       "    w2v_sim  \n",
       "0  0.999116  \n",
       "1  0.999398  \n",
       "2  0.999021  \n",
       "3  0.999651  \n",
       "4  0.000000  \n",
       "5  0.998877  \n",
       "6  0.997664  \n",
       "7  0.998489  \n",
       "8  0.998939  \n",
       "9  0.999413  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "DIM = 600\n",
    "WORKERS = 8\n",
    "WINDOW = 10\n",
    "NEGATIVE = 10\n",
    "\n",
    "id2word = gensim.corpora.Dictionary(w2v_format)\n",
    "word2id = dict((v,k) for k,v in id2word.iteritems())\n",
    "corpus = lambda: ([word.lower() for word in question if word in word2id] for question in w2v_format)\n",
    "model = Word2Vec(size=DIM, window=WINDOW, workers=WORKERS,hs=0,negative=NEGATIVE)\n",
    "model.build_vocab(corpus())\n",
    "model.train(corpus(), total_words=model.corpus_count, epochs=model.epochs)\n",
    "#Done training the model\n",
    "model.init_sims(replace=True)\n",
    "# pickle.dump(model, open(\"tmp/w2v1_model.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateVec(model, sent, numFeatures):\n",
    "    featureVec = np.zeros((numFeatures,), dtype=\"float32\")\n",
    "    num_words = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in sent:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    featureVec = np.divide(featureVec, num_words)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherly/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/Users/sherly/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "df['w2v_score'] = df.apply(lambda x: cosine_similarity(\n",
    "    generateVec(model, x['org_question'], DIM),\n",
    "    generateVec(model, x['question'], DIM)\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherly/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/Users/sherly/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "df['w2v_sub_score'] = df.apply(lambda x: cosine_similarity(\n",
    "    generateVec(model, x['org_subject'], DIM),\n",
    "    generateVec(model, x['subject'], DIM)\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORGQ_ID</th>\n",
       "      <th>org_subject</th>\n",
       "      <th>org_question</th>\n",
       "      <th>threadId</th>\n",
       "      <th>subject</th>\n",
       "      <th>question</th>\n",
       "      <th>relevance</th>\n",
       "      <th>score</th>\n",
       "      <th>w2v_sim</th>\n",
       "      <th>w2v_score</th>\n",
       "      <th>w2v_sub_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>place find scented massage oils qatar</td>\n",
       "      <td>PerfectMatch</td>\n",
       "      <td>0.320963</td>\n",
       "      <td>0.999116</td>\n",
       "      <td>0.845747</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R6</td>\n",
       "      <td>philipino massage center</td>\n",
       "      <td>one tell place good massage drom philipinies y...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.318438</td>\n",
       "      <td>0.999398</td>\n",
       "      <td>0.852600</td>\n",
       "      <td>0.706746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R8</td>\n",
       "      <td>best place massage</td>\n",
       "      <td>tell best place go massage mind want spend qr ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.234103</td>\n",
       "      <td>0.999021</td>\n",
       "      <td>0.885455</td>\n",
       "      <td>0.775217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R10</td>\n",
       "      <td>body massage</td>\n",
       "      <td>see lot massage center dont one better someone...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.483658</td>\n",
       "      <td>0.999651</td>\n",
       "      <td>0.834709</td>\n",
       "      <td>0.760214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R22</td>\n",
       "      <td>attracts</td>\n",
       "      <td>attracts</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.598539</td>\n",
       "      <td>0.412908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R25</td>\n",
       "      <td>got joking seen shop downtown manama</td>\n",
       "      <td>img assist nid title placenta cream desc link ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.855783</td>\n",
       "      <td>0.668123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R27</td>\n",
       "      <td>blackheads</td>\n",
       "      <td>suggestions get rid</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997664</td>\n",
       "      <td>0.929298</td>\n",
       "      <td>0.684770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R32</td>\n",
       "      <td>get tea tree oil</td>\n",
       "      <td>someone please advise husband wants get tea tr...</td>\n",
       "      <td>PerfectMatch</td>\n",
       "      <td>0.184031</td>\n",
       "      <td>0.998489</td>\n",
       "      <td>0.808488</td>\n",
       "      <td>0.799709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R43</td>\n",
       "      <td>strong migraine pain</td>\n",
       "      <td>plz help living hell days tried kind medicine ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.820462</td>\n",
       "      <td>0.672715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R46</td>\n",
       "      <td>garlic oil</td>\n",
       "      <td>someone please tell find garlic oil qatar hear...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.218388</td>\n",
       "      <td>0.999413</td>\n",
       "      <td>0.823690</td>\n",
       "      <td>0.850235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ORGQ_ID  org_subject          org_question threadId  \\\n",
       "0      Q1  massage oil  buy good oil massage    Q1_R1   \n",
       "1      Q1  massage oil  buy good oil massage    Q1_R6   \n",
       "2      Q1  massage oil  buy good oil massage    Q1_R8   \n",
       "3      Q1  massage oil  buy good oil massage   Q1_R10   \n",
       "4      Q1  massage oil  buy good oil massage   Q1_R22   \n",
       "5      Q1  massage oil  buy good oil massage   Q1_R25   \n",
       "6      Q1  massage oil  buy good oil massage   Q1_R27   \n",
       "7      Q1  massage oil  buy good oil massage   Q1_R32   \n",
       "8      Q1  massage oil  buy good oil massage   Q1_R43   \n",
       "9      Q1  massage oil  buy good oil massage   Q1_R46   \n",
       "\n",
       "                                subject  \\\n",
       "0                           massage oil   \n",
       "1              philipino massage center   \n",
       "2                    best place massage   \n",
       "3                          body massage   \n",
       "4                              attracts   \n",
       "5  got joking seen shop downtown manama   \n",
       "6                            blackheads   \n",
       "7                      get tea tree oil   \n",
       "8                  strong migraine pain   \n",
       "9                            garlic oil   \n",
       "\n",
       "                                            question     relevance     score  \\\n",
       "0              place find scented massage oils qatar  PerfectMatch  0.320963   \n",
       "1  one tell place good massage drom philipinies y...      Relevant  0.318438   \n",
       "2  tell best place go massage mind want spend qr ...    Irrelevant  0.234103   \n",
       "3  see lot massage center dont one better someone...      Relevant  0.483658   \n",
       "4                                           attracts    Irrelevant  0.000000   \n",
       "5  img assist nid title placenta cream desc link ...    Irrelevant  0.000000   \n",
       "6                                suggestions get rid    Irrelevant  0.000000   \n",
       "7  someone please advise husband wants get tea tr...  PerfectMatch  0.184031   \n",
       "8  plz help living hell days tried kind medicine ...    Irrelevant  0.000000   \n",
       "9  someone please tell find garlic oil qatar hear...    Irrelevant  0.218388   \n",
       "\n",
       "    w2v_sim  w2v_score  w2v_sub_score  \n",
       "0  0.999116   0.845747       1.000000  \n",
       "1  0.999398   0.852600       0.706746  \n",
       "2  0.999021   0.885455       0.775217  \n",
       "3  0.999651   0.834709       0.760214  \n",
       "4  0.000000   0.598539       0.412908  \n",
       "5  0.998877   0.855783       0.668123  \n",
       "6  0.997664   0.929298       0.684770  \n",
       "7  0.998489   0.808488       0.799709  \n",
       "8  0.998939   0.820462       0.672715  \n",
       "9  0.999413   0.823690       0.850235  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lsi'></a>\n",
    "### 3.5 Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be utilising the library gensim to perform LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for txt in w2v_format:\n",
    "    for t in txt:\n",
    "        frequency[t] += 1\n",
    "        \n",
    "texts = [[token for token in text if frequency[token]>1]\n",
    "        for text in w2v_format]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 18:43:42,858 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-11-13 18:43:43,575 : INFO : built Dictionary(4707 unique tokens: ['buy', 'find', 'good', 'massage', 'oil']...) from 2669 documents (total 100204 corpus positions)\n",
      "2019-11-13 18:43:43,583 : INFO : saving Dictionary object under tmp/question.dict, separately None\n",
      "2019-11-13 18:43:43,664 : INFO : saved tmp/question.dict\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('tmp/question.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 18:43:44,273 : INFO : storing corpus in Matrix Market format to tmp/question.mm\n",
      "2019-11-13 18:43:44,293 : INFO : saving sparse matrix to tmp/question.mm\n",
      "2019-11-13 18:43:44,303 : INFO : PROGRESS: saving document #0\n",
      "2019-11-13 18:43:44,885 : INFO : PROGRESS: saving document #1000\n",
      "2019-11-13 18:43:45,365 : INFO : PROGRESS: saving document #2000\n",
      "2019-11-13 18:43:45,558 : INFO : saved 2669x4707 matrix, density=0.691% (86861/12562983)\n",
      "2019-11-13 18:43:45,641 : INFO : saving MmCorpus index to tmp/question.mm.index\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('tmp/question.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can load back the dict and corpus too\n",
    "# dictionary = corpora.Dictionary.load('tmp/question.dict')\n",
    "# corpus = corpora.MmCorpus('tmp/question.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 18:43:45,787 : INFO : using serial LSI version on this node\n",
      "2019-11-13 18:43:45,819 : INFO : updating model with new documents\n",
      "2019-11-13 18:43:45,828 : INFO : preparing a new chunk of documents\n",
      "2019-11-13 18:43:46,098 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-11-13 18:43:46,101 : INFO : 1st phase: constructing (4707, 300) action matrix\n",
      "2019-11-13 18:43:46,515 : INFO : orthonormalizing (4707, 300) action matrix\n",
      "2019-11-13 18:43:49,501 : INFO : 2nd phase: running dense svd on (300, 2669) matrix\n",
      "2019-11-13 18:43:50,967 : INFO : computing the final decomposition\n",
      "2019-11-13 18:43:51,000 : INFO : keeping 200 factors (discarding 9.575% of energy spectrum)\n",
      "2019-11-13 18:43:51,077 : INFO : processed documents up to #2669\n",
      "2019-11-13 18:43:51,103 : INFO : topic #0(106.660): 0.425*\"visa\" + 0.406*\"qatar\" + 0.225*\"doha\" + 0.211*\"know\" + 0.207*\"get\" + 0.169*\"visit\" + 0.168*\"please\" + 0.145*\"would\" + 0.137*\"anyone\" + 0.137*\"help\"\n",
      "2019-11-13 18:43:51,116 : INFO : topic #1(67.921): -0.691*\"visa\" + 0.300*\"doha\" + -0.256*\"visit\" + 0.185*\"would\" + 0.165*\"know\" + 0.148*\"like\" + -0.131*\"family\" + 0.128*\"good\" + 0.126*\"anyone\" + 0.093*\"school\"\n",
      "2019-11-13 18:43:51,127 : INFO : topic #2(49.472): 0.749*\"qatar\" + -0.411*\"doha\" + -0.163*\"visa\" + -0.136*\"family\" + 0.115*\"company\" + -0.104*\"anyone\" + -0.102*\"know\" + -0.092*\"visit\" + -0.079*\"please\" + -0.078*\"school\"\n",
      "2019-11-13 18:43:51,180 : INFO : topic #3(42.137): -0.384*\"know\" + 0.311*\"doha\" + 0.268*\"would\" + -0.241*\"license\" + 0.229*\"like\" + -0.226*\"driving\" + -0.214*\"please\" + 0.210*\"water\" + -0.186*\"car\" + -0.156*\"one\"\n",
      "2019-11-13 18:43:51,235 : INFO : topic #4(41.232): -0.438*\"doha\" + 0.396*\"like\" + 0.394*\"water\" + 0.319*\"would\" + 0.202*\"need\" + 0.180*\"know\" + -0.142*\"qatar\" + 0.132*\"bottled\" + -0.129*\"school\" + 0.104*\"prices\"\n"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 18:43:51,274 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2019-11-13 18:43:52,137 : INFO : creating matrix with 2669 documents and 200 features\n"
     ]
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 0.9941629), (5, 0.9146293), (6, 0.79458326), (1, 0.739517), (0, 0.71737975), (3, 0.61968863), (9, 0.5743849), (7, 0.5676053), (814, 0.5496509), (1021, 0.5422623)]\n"
     ]
    }
   ],
   "source": [
    "doc = df.loc[0].org_question\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]\n",
    "sims = index[vec_lsi]\n",
    "enum_sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "\n",
    "print(enum_sims[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sim(x):\n",
    "    vec_bow = dictionary.doc2bow(x['org_question'].lower().split())\n",
    "    vec_lsi = lsi[vec_bow]\n",
    "    sims = index[vec_lsi]\n",
    "    for idx, score in enumerate(sims):\n",
    "        if idx == x['row_index']:\n",
    "            return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['row_index'] = df.index\n",
    "df['lsi_score'] = df.apply(lambda x: gen_sim(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORGQ_ID</th>\n",
       "      <th>org_subject</th>\n",
       "      <th>org_question</th>\n",
       "      <th>threadId</th>\n",
       "      <th>subject</th>\n",
       "      <th>question</th>\n",
       "      <th>relevance</th>\n",
       "      <th>score</th>\n",
       "      <th>w2v_sim</th>\n",
       "      <th>w2v_score</th>\n",
       "      <th>w2v_sub_score</th>\n",
       "      <th>row_index</th>\n",
       "      <th>lsi_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>place find scented massage oils qatar</td>\n",
       "      <td>PerfectMatch</td>\n",
       "      <td>0.320963</td>\n",
       "      <td>0.999116</td>\n",
       "      <td>0.845747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.717380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R6</td>\n",
       "      <td>philipino massage center</td>\n",
       "      <td>one tell place good massage drom philipinies y...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.318438</td>\n",
       "      <td>0.999398</td>\n",
       "      <td>0.852600</td>\n",
       "      <td>0.706746</td>\n",
       "      <td>1</td>\n",
       "      <td>0.739517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R8</td>\n",
       "      <td>best place massage</td>\n",
       "      <td>tell best place go massage mind want spend qr ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.234103</td>\n",
       "      <td>0.999021</td>\n",
       "      <td>0.885455</td>\n",
       "      <td>0.775217</td>\n",
       "      <td>2</td>\n",
       "      <td>0.494614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R10</td>\n",
       "      <td>body massage</td>\n",
       "      <td>see lot massage center dont one better someone...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.483658</td>\n",
       "      <td>0.999651</td>\n",
       "      <td>0.834709</td>\n",
       "      <td>0.760214</td>\n",
       "      <td>3</td>\n",
       "      <td>0.619689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R22</td>\n",
       "      <td>attracts</td>\n",
       "      <td>attracts</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.598539</td>\n",
       "      <td>0.412908</td>\n",
       "      <td>4</td>\n",
       "      <td>0.994163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R25</td>\n",
       "      <td>got joking seen shop downtown manama</td>\n",
       "      <td>img assist nid title placenta cream desc link ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.855783</td>\n",
       "      <td>0.668123</td>\n",
       "      <td>5</td>\n",
       "      <td>0.914629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R27</td>\n",
       "      <td>blackheads</td>\n",
       "      <td>suggestions get rid</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997664</td>\n",
       "      <td>0.929298</td>\n",
       "      <td>0.684770</td>\n",
       "      <td>6</td>\n",
       "      <td>0.794583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R32</td>\n",
       "      <td>get tea tree oil</td>\n",
       "      <td>someone please advise husband wants get tea tr...</td>\n",
       "      <td>PerfectMatch</td>\n",
       "      <td>0.184031</td>\n",
       "      <td>0.998489</td>\n",
       "      <td>0.808488</td>\n",
       "      <td>0.799709</td>\n",
       "      <td>7</td>\n",
       "      <td>0.567605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R43</td>\n",
       "      <td>strong migraine pain</td>\n",
       "      <td>plz help living hell days tried kind medicine ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.820462</td>\n",
       "      <td>0.672715</td>\n",
       "      <td>8</td>\n",
       "      <td>0.426919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q1</td>\n",
       "      <td>massage oil</td>\n",
       "      <td>buy good oil massage</td>\n",
       "      <td>Q1_R46</td>\n",
       "      <td>garlic oil</td>\n",
       "      <td>someone please tell find garlic oil qatar hear...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.218388</td>\n",
       "      <td>0.999413</td>\n",
       "      <td>0.823690</td>\n",
       "      <td>0.850235</td>\n",
       "      <td>9</td>\n",
       "      <td>0.574385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ORGQ_ID  org_subject          org_question threadId  \\\n",
       "0      Q1  massage oil  buy good oil massage    Q1_R1   \n",
       "1      Q1  massage oil  buy good oil massage    Q1_R6   \n",
       "2      Q1  massage oil  buy good oil massage    Q1_R8   \n",
       "3      Q1  massage oil  buy good oil massage   Q1_R10   \n",
       "4      Q1  massage oil  buy good oil massage   Q1_R22   \n",
       "5      Q1  massage oil  buy good oil massage   Q1_R25   \n",
       "6      Q1  massage oil  buy good oil massage   Q1_R27   \n",
       "7      Q1  massage oil  buy good oil massage   Q1_R32   \n",
       "8      Q1  massage oil  buy good oil massage   Q1_R43   \n",
       "9      Q1  massage oil  buy good oil massage   Q1_R46   \n",
       "\n",
       "                                subject  \\\n",
       "0                           massage oil   \n",
       "1              philipino massage center   \n",
       "2                    best place massage   \n",
       "3                          body massage   \n",
       "4                              attracts   \n",
       "5  got joking seen shop downtown manama   \n",
       "6                            blackheads   \n",
       "7                      get tea tree oil   \n",
       "8                  strong migraine pain   \n",
       "9                            garlic oil   \n",
       "\n",
       "                                            question     relevance     score  \\\n",
       "0              place find scented massage oils qatar  PerfectMatch  0.320963   \n",
       "1  one tell place good massage drom philipinies y...      Relevant  0.318438   \n",
       "2  tell best place go massage mind want spend qr ...    Irrelevant  0.234103   \n",
       "3  see lot massage center dont one better someone...      Relevant  0.483658   \n",
       "4                                           attracts    Irrelevant  0.000000   \n",
       "5  img assist nid title placenta cream desc link ...    Irrelevant  0.000000   \n",
       "6                                suggestions get rid    Irrelevant  0.000000   \n",
       "7  someone please advise husband wants get tea tr...  PerfectMatch  0.184031   \n",
       "8  plz help living hell days tried kind medicine ...    Irrelevant  0.000000   \n",
       "9  someone please tell find garlic oil qatar hear...    Irrelevant  0.218388   \n",
       "\n",
       "    w2v_sim  w2v_score  w2v_sub_score  row_index  lsi_score  \n",
       "0  0.999116   0.845747       1.000000          0   0.717380  \n",
       "1  0.999398   0.852600       0.706746          1   0.739517  \n",
       "2  0.999021   0.885455       0.775217          2   0.494614  \n",
       "3  0.999651   0.834709       0.760214          3   0.619689  \n",
       "4  0.000000   0.598539       0.412908          4   0.994163  \n",
       "5  0.998877   0.855783       0.668123          5   0.914629  \n",
       "6  0.997664   0.929298       0.684770          6   0.794583  \n",
       "7  0.998489   0.808488       0.799709          7   0.567605  \n",
       "8  0.998939   0.820462       0.672715          8   0.426919  \n",
       "9  0.999413   0.823690       0.850235          9   0.574385  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fasttext'></a>\n",
    "### 3.6 FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fasttext\n",
    "import fastText\n",
    "\n",
    "# Skipgram model\n",
    "model = fastText.train_unsupervised('train.txt', model='skipgram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>',\n",
       " 'qatar',\n",
       " 'visa',\n",
       " 'doha',\n",
       " 'know',\n",
       " 'please',\n",
       " 'get',\n",
       " 'anyone',\n",
       " 'would',\n",
       " 'one']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method _FastText.get_words of <fastText.FastText._FastText object at 0x1056499b0>>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the tokens \n",
    "words = []\n",
    "for word in model.get_words():\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 2676\n",
      "Dimension of a word vector: 100\n",
      "Vector components of a word: [-0.0319672   0.1319892   0.31514543  0.410413   -0.1447973   0.04152701\n",
      "  0.09371077  0.01562968  0.1512818   0.14957687 -0.01333168  0.08096377\n",
      " -0.07832588  0.24669628 -0.04287419 -0.07149091 -0.1387505   0.2956999\n",
      "  0.01943246  0.03656456  0.07951707 -0.11457002 -0.06516531 -0.16427673\n",
      " -0.04190664 -0.02716281 -0.10165581 -0.13693957 -0.08325977 -0.19285879\n",
      " -0.08619766  0.13417538 -0.2542087  -0.13881095  0.0810195   0.11718099\n",
      "  0.0253346  -0.30254847 -0.10877774 -0.07203186  0.11552175  0.06549048\n",
      "  0.36798212  0.06593114 -0.12190998  0.23032668 -0.3667291  -0.14014255\n",
      "  0.19403766 -0.06437854 -0.11028724  0.00516628  0.11386647  0.00657951\n",
      " -0.13765365 -0.0285877  -0.18273328  0.2647317   0.08529811 -0.01537095\n",
      " -0.23491588  0.06719963  0.17836581 -0.02758135 -0.03991823 -0.23159184\n",
      " -0.20494325 -0.05576655  0.12461594 -0.07800097  0.01072029 -0.02262745\n",
      " -0.23066637  0.04385775 -0.06628027 -0.02176488 -0.24662724 -0.03221891\n",
      "  0.06524684  0.05948818 -0.02863715 -0.22110938 -0.15031967  0.08935812\n",
      " -0.05500029  0.14479908 -0.02208875 -0.18465653  0.04692621 -0.09904585\n",
      " -0.08002793  0.05153101 -0.11708701 -0.199392   -0.06623735 -0.03809406\n",
      " -0.3665849  -0.15771277 -0.03631447 -0.02789003]\n"
     ]
    }
   ],
   "source": [
    "# Printing out number of tokens available\n",
    "print(\"Number of Tokens: {}\".format(len(words)))\n",
    "\n",
    "# Printing out the dimension of a word vector \n",
    "print(\"Dimension of a word vector: {}\".format(\n",
    "    len(model.get_word_vector(words[0]))\n",
    "))\n",
    "\n",
    "# Print out the vector of a word \n",
    "print(\"Vector components of a word: {}\".format(\n",
    "    model.get_word_vector(words[0])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasttext_sim(model, x):\n",
    "    return cosine_similarity(model.get_word_vector(x['org_question']),\n",
    "                            model.get_word_vector(x['question']))\n",
    "df['ft_sim'] = df.apply(lambda x: get_fasttext_sim(model, x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORGQ_ID</th>\n",
       "      <th>relevance</th>\n",
       "      <th>score</th>\n",
       "      <th>w2v_score</th>\n",
       "      <th>w2v_sub_score</th>\n",
       "      <th>lsi_score</th>\n",
       "      <th>ft_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>PerfectMatch</td>\n",
       "      <td>0.320963</td>\n",
       "      <td>0.845747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.717380</td>\n",
       "      <td>0.946557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.318438</td>\n",
       "      <td>0.852600</td>\n",
       "      <td>0.706746</td>\n",
       "      <td>0.739517</td>\n",
       "      <td>0.877368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.234103</td>\n",
       "      <td>0.885455</td>\n",
       "      <td>0.775217</td>\n",
       "      <td>0.494614</td>\n",
       "      <td>0.910362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.483658</td>\n",
       "      <td>0.834709</td>\n",
       "      <td>0.760214</td>\n",
       "      <td>0.619689</td>\n",
       "      <td>0.929131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.598539</td>\n",
       "      <td>0.412908</td>\n",
       "      <td>0.994163</td>\n",
       "      <td>0.866588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.855783</td>\n",
       "      <td>0.668123</td>\n",
       "      <td>0.914629</td>\n",
       "      <td>0.946325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.929298</td>\n",
       "      <td>0.684770</td>\n",
       "      <td>0.794583</td>\n",
       "      <td>0.720323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q1</td>\n",
       "      <td>PerfectMatch</td>\n",
       "      <td>0.184031</td>\n",
       "      <td>0.808488</td>\n",
       "      <td>0.799709</td>\n",
       "      <td>0.567605</td>\n",
       "      <td>0.701124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820462</td>\n",
       "      <td>0.672715</td>\n",
       "      <td>0.426919</td>\n",
       "      <td>0.793625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.218388</td>\n",
       "      <td>0.823690</td>\n",
       "      <td>0.850235</td>\n",
       "      <td>0.574385</td>\n",
       "      <td>0.918123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Q2</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.272329</td>\n",
       "      <td>0.991504</td>\n",
       "      <td>0.900761</td>\n",
       "      <td>0.822276</td>\n",
       "      <td>0.977502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Q2</td>\n",
       "      <td>PerfectMatch</td>\n",
       "      <td>0.203774</td>\n",
       "      <td>0.971287</td>\n",
       "      <td>0.941512</td>\n",
       "      <td>0.876908</td>\n",
       "      <td>0.967194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Q2</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.282352</td>\n",
       "      <td>0.987589</td>\n",
       "      <td>0.978697</td>\n",
       "      <td>0.826627</td>\n",
       "      <td>0.980405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Q2</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.301733</td>\n",
       "      <td>0.981939</td>\n",
       "      <td>0.944490</td>\n",
       "      <td>0.806183</td>\n",
       "      <td>0.986246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Q2</td>\n",
       "      <td>PerfectMatch</td>\n",
       "      <td>0.233468</td>\n",
       "      <td>0.983029</td>\n",
       "      <td>0.971786</td>\n",
       "      <td>0.836678</td>\n",
       "      <td>0.964179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Q2</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.144473</td>\n",
       "      <td>0.985511</td>\n",
       "      <td>0.955111</td>\n",
       "      <td>0.898566</td>\n",
       "      <td>0.963838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Q2</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.462831</td>\n",
       "      <td>0.991403</td>\n",
       "      <td>0.910953</td>\n",
       "      <td>0.927152</td>\n",
       "      <td>0.970790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Q2</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.197992</td>\n",
       "      <td>0.991095</td>\n",
       "      <td>0.921388</td>\n",
       "      <td>0.892396</td>\n",
       "      <td>0.954649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Q2</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.227239</td>\n",
       "      <td>0.981968</td>\n",
       "      <td>0.585500</td>\n",
       "      <td>0.826710</td>\n",
       "      <td>0.975758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Q2</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.102490</td>\n",
       "      <td>0.980807</td>\n",
       "      <td>0.952669</td>\n",
       "      <td>0.865697</td>\n",
       "      <td>0.952501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ORGQ_ID     relevance     score  w2v_score  w2v_sub_score  lsi_score  \\\n",
       "0       Q1  PerfectMatch  0.320963   0.845747       1.000000   0.717380   \n",
       "1       Q1      Relevant  0.318438   0.852600       0.706746   0.739517   \n",
       "2       Q1    Irrelevant  0.234103   0.885455       0.775217   0.494614   \n",
       "3       Q1      Relevant  0.483658   0.834709       0.760214   0.619689   \n",
       "4       Q1    Irrelevant  0.000000   0.598539       0.412908   0.994163   \n",
       "5       Q1    Irrelevant  0.000000   0.855783       0.668123   0.914629   \n",
       "6       Q1    Irrelevant  0.000000   0.929298       0.684770   0.794583   \n",
       "7       Q1  PerfectMatch  0.184031   0.808488       0.799709   0.567605   \n",
       "8       Q1    Irrelevant  0.000000   0.820462       0.672715   0.426919   \n",
       "9       Q1    Irrelevant  0.218388   0.823690       0.850235   0.574385   \n",
       "10      Q2      Relevant  0.272329   0.991504       0.900761   0.822276   \n",
       "11      Q2  PerfectMatch  0.203774   0.971287       0.941512   0.876908   \n",
       "12      Q2      Relevant  0.282352   0.987589       0.978697   0.826627   \n",
       "13      Q2      Relevant  0.301733   0.981939       0.944490   0.806183   \n",
       "14      Q2  PerfectMatch  0.233468   0.983029       0.971786   0.836678   \n",
       "15      Q2    Irrelevant  0.144473   0.985511       0.955111   0.898566   \n",
       "16      Q2      Relevant  0.462831   0.991403       0.910953   0.927152   \n",
       "17      Q2    Irrelevant  0.197992   0.991095       0.921388   0.892396   \n",
       "18      Q2      Relevant  0.227239   0.981968       0.585500   0.826710   \n",
       "19      Q2      Relevant  0.102490   0.980807       0.952669   0.865697   \n",
       "\n",
       "      ft_sim  \n",
       "0   0.946557  \n",
       "1   0.877368  \n",
       "2   0.910362  \n",
       "3   0.929131  \n",
       "4   0.866588  \n",
       "5   0.946325  \n",
       "6   0.720323  \n",
       "7   0.701124  \n",
       "8   0.793625  \n",
       "9   0.918123  \n",
       "10  0.977502  \n",
       "11  0.967194  \n",
       "12  0.980405  \n",
       "13  0.986246  \n",
       "14  0.964179  \n",
       "15  0.963838  \n",
       "16  0.970790  \n",
       "17  0.954649  \n",
       "18  0.975758  \n",
       "19  0.952501  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"ORGQ_ID\", \"relevance\", \"score\", \"w2v_score\",\n",
    "    \"w2v_sub_score\", \"lsi_score\", \"ft_sim\"]].head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='evaluation'></a>\n",
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = df[[\"ORGQ_ID\", \"relevance\", \"score\", \"w2v_score\",\n",
    "    \"w2v_sub_score\", \"lsi_score\", \"ft_sim\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherly/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/sherly/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "eval_df['rel'] = eval_df.relevance.apply(\n",
    "    lambda x: 1 if x in ['PerfectMatch', 'Relevant'] else 0)\n",
    "eval_df['w2vsimclass'] = eval_df.w2v_score.apply(lambda x: 1 if x > 0.85 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORGQ_ID</th>\n",
       "      <th>relevance</th>\n",
       "      <th>score</th>\n",
       "      <th>w2v_score</th>\n",
       "      <th>w2v_sub_score</th>\n",
       "      <th>lsi_score</th>\n",
       "      <th>ft_sim</th>\n",
       "      <th>rel</th>\n",
       "      <th>w2vsimclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>PerfectMatch</td>\n",
       "      <td>0.320963</td>\n",
       "      <td>0.845747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.717380</td>\n",
       "      <td>0.946557</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.318438</td>\n",
       "      <td>0.852600</td>\n",
       "      <td>0.706746</td>\n",
       "      <td>0.739517</td>\n",
       "      <td>0.877368</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.234103</td>\n",
       "      <td>0.885455</td>\n",
       "      <td>0.775217</td>\n",
       "      <td>0.494614</td>\n",
       "      <td>0.910362</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.483658</td>\n",
       "      <td>0.834709</td>\n",
       "      <td>0.760214</td>\n",
       "      <td>0.619689</td>\n",
       "      <td>0.929131</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.598539</td>\n",
       "      <td>0.412908</td>\n",
       "      <td>0.994163</td>\n",
       "      <td>0.866588</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.855783</td>\n",
       "      <td>0.668123</td>\n",
       "      <td>0.914629</td>\n",
       "      <td>0.946325</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.929298</td>\n",
       "      <td>0.684770</td>\n",
       "      <td>0.794583</td>\n",
       "      <td>0.720323</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q1</td>\n",
       "      <td>PerfectMatch</td>\n",
       "      <td>0.184031</td>\n",
       "      <td>0.808488</td>\n",
       "      <td>0.799709</td>\n",
       "      <td>0.567605</td>\n",
       "      <td>0.701124</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820462</td>\n",
       "      <td>0.672715</td>\n",
       "      <td>0.426919</td>\n",
       "      <td>0.793625</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0.218388</td>\n",
       "      <td>0.823690</td>\n",
       "      <td>0.850235</td>\n",
       "      <td>0.574385</td>\n",
       "      <td>0.918123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ORGQ_ID     relevance     score  w2v_score  w2v_sub_score  lsi_score  \\\n",
       "0      Q1  PerfectMatch  0.320963   0.845747       1.000000   0.717380   \n",
       "1      Q1      Relevant  0.318438   0.852600       0.706746   0.739517   \n",
       "2      Q1    Irrelevant  0.234103   0.885455       0.775217   0.494614   \n",
       "3      Q1      Relevant  0.483658   0.834709       0.760214   0.619689   \n",
       "4      Q1    Irrelevant  0.000000   0.598539       0.412908   0.994163   \n",
       "5      Q1    Irrelevant  0.000000   0.855783       0.668123   0.914629   \n",
       "6      Q1    Irrelevant  0.000000   0.929298       0.684770   0.794583   \n",
       "7      Q1  PerfectMatch  0.184031   0.808488       0.799709   0.567605   \n",
       "8      Q1    Irrelevant  0.000000   0.820462       0.672715   0.426919   \n",
       "9      Q1    Irrelevant  0.218388   0.823690       0.850235   0.574385   \n",
       "\n",
       "     ft_sim  rel  w2vsimclass  \n",
       "0  0.946557    1            0  \n",
       "1  0.877368    1            1  \n",
       "2  0.910362    0            1  \n",
       "3  0.929131    1            0  \n",
       "4  0.866588    0            0  \n",
       "5  0.946325    0            1  \n",
       "6  0.720323    0            1  \n",
       "7  0.701124    1            0  \n",
       "8  0.793625    0            0  \n",
       "9  0.918123    0            0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets say 0.8 sim is relevant\n",
    "eval_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mapval = mapk(eval_df.groupby([\"ORGQ_ID\"])['rel'].apply(list).values,\n",
    "    eval_df.groupby([\"ORGQ_ID\"])['w2vsimclass'].apply(list).values, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09102565245823672"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mapval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 18:46:50,690 : INFO : loading projection weights from GoogleNews-vectors-negative300.bin.gz\n",
      "2019-11-13 18:55:07,423 : INFO : loaded (3000000, 300) matrix from GoogleNews-vectors-negative300.bin.gz\n",
      "2019-11-13 18:55:07,472 : INFO : loading projection weights from GoogleNews-vectors-negative300.bin.gz\n",
      "2019-11-13 19:01:41,789 : INFO : loaded (3000000, 300) matrix from GoogleNews-vectors-negative300.bin.gz\n",
      "2019-11-13 19:01:41,852 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "\n",
    "from gensim import models\n",
    "gensim_model = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "norm_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "norm_model.init_sims(replace=True)\n",
    "\n",
    "def wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return gensim_model.wmdistance(s1, s2)\n",
    "\n",
    "\n",
    "def norm_wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return norm_model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def sentence2vector(s):\n",
    "    words = str(s).lower()\n",
    "#     .decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(gensim_model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    # normalized vector\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "def gen_features(data):\n",
    "    data['len_q1'] = data.question1.apply(lambda x: len(str(x)))\n",
    "    data['len_q2'] = data.question2.apply(lambda x: len(str(x)))\n",
    "    data['diff_len'] = data.len_q1 - data.len_q2\n",
    "    \n",
    "    data['len_char_q1'] = data.question1.apply(\n",
    "        lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "    data['len_char_q2'] = data.question2.apply(\n",
    "        lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "    data['len_word_q1'] = data.question1.apply(\n",
    "        lambda x: len(str(x).split()))\n",
    "    data['len_word_q2'] = data.question2.apply(\n",
    "        lambda x: len(str(x).split()))\n",
    "    data['common_words'] = data.apply(\n",
    "        lambda x: len(set(str(x['question1']).lower().split()) \\\n",
    "                      .intersection(set(str(x['question2']).lower() \\\n",
    "                      .split()))), axis=1)\n",
    "    \n",
    "    data['fuzz_qratio'] = data.apply(\n",
    "        lambda x: fuzz.QRatio(str(x['question1']),\n",
    "                              str(x['question2'])), axis=1)\n",
    "    data['fuzz_WRatio'] = data.apply(\n",
    "        lambda x: fuzz.WRatio(str(x['question1']),\n",
    "                              str(x['question2'])), axis=1)\n",
    "    data['fuzz_partial_ratio'] = data.apply(\n",
    "        lambda x: fuzz.partial_ratio(str(x['question1']),\n",
    "                                     str(x['question2'])), axis=1)\n",
    "    data['fuzz_partial_token_set_ratio'] = data.apply(\n",
    "        lambda x: fuzz.partial_token_set_ratio(str(x['question1']),\n",
    "                                               str(x['question2'])), axis=1)\n",
    "    \n",
    "    data['fuzz_partial_token_sort_ratio'] = data.apply(\n",
    "        lambda x: fuzz.partial_token_sort_ratio(str(x['question1']),\n",
    "                                                str(x['question2'])), axis=1)\n",
    "    data['fuzz_token_set_ratio'] = data.apply(\n",
    "        lambda x: fuzz.token_set_ratio(str(x['question1']),\n",
    "                                       str(x['question2'])), axis=1)\n",
    "    data['fuzz_token_sort_ratio'] = data.apply(\n",
    "        lambda x: fuzz.token_sort_ratio(str(x['question1']),\n",
    "                                        str(x['question2'])), axis=1)\n",
    "    \n",
    "    data['wmd'] = data.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "\n",
    "\n",
    "    data['norm_wmd'] = data.apply(lambda x: norm_wmd(x['question1'],\n",
    "                                                     x['question2']), axis=1)\n",
    "\n",
    "    # generate question vectors\n",
    "    question1_vectors = np.zeros((data.shape[0], 300))\n",
    "    error_count = 0\n",
    "\n",
    "    for i, q in tqdm(enumerate(data.question1.values)):\n",
    "        question1_vectors[i, :] = sentence2vector(q)\n",
    "\n",
    "    question2_vectors  = np.zeros((data.shape[0], 300))\n",
    "    for i, q in tqdm(enumerate(data.question2.values)):\n",
    "        question2_vectors[i, :] = sentence2vector(q)\n",
    "\n",
    "    data['cosine_distance'] = [cosine(x, y) for (x, y) in zip(\n",
    "        np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(\n",
    "        np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(\n",
    "        np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['canberra_distance'] = [canberra(x, y) for (x, y) in zip(\n",
    "        np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(\n",
    "        np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(\n",
    "        np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(\n",
    "        np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "    data['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "    data['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "    data['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['question1'] = df_copy['org_question']\n",
    "df_copy['question2'] = df_copy['question']\n",
    "df_copy = df_copy[['question1', 'question2', 'relevance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global PYEMD_EXT\n",
    "import pyemd\n",
    "PYEMD_EXT = True\n",
    "\n",
    "features_df = gen_features(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['q1vec'] = features_df.question1.apply(\n",
    "    lambda q: sentence2vector(q))\n",
    "features_df['q2vec'] = features_df.question2.apply(\n",
    "    lambda q: sentence2vector(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question1', 'question2', 'relevance', 'len_q1', 'len_q2', 'diff_len',\n",
       "       'len_char_q1', 'len_char_q2', 'len_word_q1', 'len_word_q2',\n",
       "       'common_words', 'fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio',\n",
       "       'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio',\n",
       "       'fuzz_token_set_ratio', 'fuzz_token_sort_ratio', 'wmd', 'norm_wmd',\n",
       "       'cosine_distance', 'cityblock_distance', 'jaccard_distance',\n",
       "       'canberra_distance', 'euclidean_distance', 'minkowski_distance',\n",
       "       'braycurtis_distance', 'skew_q1vec', 'skew_q2vec', 'kur_q1vec',\n",
       "       'kur_q2vec', 'q1vec', 'q2vec'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = features_df[['len_q1', 'len_q2', 'diff_len',\n",
    "       'len_char_q1', 'len_char_q2', 'len_word_q1', 'len_word_q2',\n",
    "       'common_words', 'fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio',\n",
    "       'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio',\n",
    "       'fuzz_token_set_ratio', 'fuzz_token_sort_ratio', 'wmd', 'norm_wmd',\n",
    "       'cosine_distance', 'cityblock_distance', 'jaccard_distance',\n",
    "       'canberra_distance', 'euclidean_distance', 'minkowski_distance',\n",
    "       'braycurtis_distance', 'skew_q1vec', 'skew_q2vec', 'kur_q1vec',\n",
    "       'kur_q2vec']].values\n",
    "features_df['rel'] = features_df.relevance.apply(\n",
    "    lambda x: 1 if x in ['PerfectMatch', 'Relevant'] else 0)\n",
    "\n",
    "y = features_df.rel.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 XGboost Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.86      0.77       469\n",
      "           1       0.71      0.49      0.58       332\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       801\n",
      "   macro avg       0.70      0.67      0.68       801\n",
      "weighted avg       0.70      0.70      0.69       801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model1 = xgb.XGBClassifier()\n",
    "train_model1 = model1.fit(X_train, y_train)\n",
    "pred1 = train_model1.predict(X_test)\n",
    "print('Model 1 XGboost Report\\n',(classification_report(y_test, pred1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 XGboost Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.80      0.75       469\n",
      "           1       0.65      0.55      0.59       332\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       801\n",
      "   macro avg       0.68      0.67      0.67       801\n",
      "weighted avg       0.69      0.69      0.69       801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = xgb.XGBClassifier(n_estimators=100, max_depth=8, learning_rate=0.1, subsample=0.5)\n",
    "\n",
    "train_model2 = model2.fit(X_train, y_train)\n",
    "pred2 = train_model2.predict(X_test)\n",
    "\n",
    "print('Model 2 XGboost Report\\n', (classification_report(y_test, pred2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for model 1: 70.41\n",
      "Accuracy for model 2: 69.16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy for model 1: %.2f\" % (accuracy_score(y_test, pred1) * 100))\n",
    "print(\"Accuracy for model 2: %.2f\" % (accuracy_score(y_test, pred2) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for model 3: 69.29\n"
     ]
    }
   ],
   "source": [
    "model3 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "train_model3 = model3.fit(X_train, y_train)\n",
    "pred3 = train_model3.predict(X_test)\n",
    "print(\"Accuracy for model 3: %.2f\" % (accuracy_score(y_test, pred3) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for model 4: 66.92\n"
     ]
    }
   ],
   "source": [
    "xgb2 = xgb.XGBClassifier(\n",
    " learning_rate =0.7,\n",
    " n_estimators=1000,\n",
    " max_depth=4,\n",
    " min_child_weight=6,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "train_model4 = xgb2.fit(X_train, y_train)\n",
    "pred4 = train_model4.predict(X_test)\n",
    "print(\"Accuracy for model 4: %.2f\" % (accuracy_score(y_test, pred4) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropnafea = features_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xf_train, Xf_test, yf_train, yf_test = train_test_split(\n",
    "    dropnafea[['len_q1', 'len_q2', 'diff_len',\n",
    "       'len_char_q1', 'len_char_q2', 'len_word_q1', 'len_word_q2',\n",
    "       'common_words', 'fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio',\n",
    "       'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio',\n",
    "       'fuzz_token_set_ratio', 'fuzz_token_sort_ratio', 'wmd', 'norm_wmd',\n",
    "       'cosine_distance', 'cityblock_distance', 'jaccard_distance',\n",
    "       'canberra_distance', 'euclidean_distance', 'minkowski_distance',\n",
    "       'braycurtis_distance', 'skew_q1vec', 'skew_q2vec', 'kur_q1vec',\n",
    "       'kur_q2vec']].values, dropnafea.rel.values, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherly/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forest Model: 69.26\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_model = rfc.fit(Xf_train, yf_train)\n",
    "pred8 = rfc_model.predict(Xf_test)\n",
    "print(\"Accuracy for Random Forest Model: %.2f\" % (accuracy_score(yf_test, pred8) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/babatee/intro-xgboost-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Write up and discussions can be found in the `NLP_project.pdf` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
